{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqcqeXZCmgvY",
        "outputId": "1188ec27-68dd-46c6-eed6-5f7f6565ba4e"
      },
      "id": "FqcqeXZCmgvY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "wg0XAJfrcQBY"
      },
      "id": "wg0XAJfrcQBY",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyconll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmP3-S6Gzlry",
        "outputId": "854846f8-e8f3-4ee3-c38f-e1fb8b23de1c"
      },
      "id": "WmP3-S6Gzlry",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open(\"/content/drive/MyDrive/NLP/EN-English/en_train.conll\")"
      ],
      "metadata": {
        "id": "MMAHV5QcjOtP"
      },
      "id": "MMAHV5QcjOtP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = data.read()"
      ],
      "metadata": {
        "id": "J09Fcojzj0U5"
      },
      "id": "J09Fcojzj0U5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=annotations.split('\\n')\n",
        "x[0:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHlNW0jakACz",
        "outputId": "7c9f7c7b-acaa-42ad-98e1-f64b42ea2059"
      },
      "id": "zHlNW0jakACz",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# id fc76cef4-3a91-4394-b6d4-be7c4ed840d5\\tdomain=train',\n",
              " 'his _ _ O',\n",
              " 'playlist _ _ O',\n",
              " 'includes _ _ O',\n",
              " 'sonny _ _ B-PER',\n",
              " 'sharrock _ _ I-PER',\n",
              " ', _ _ O',\n",
              " 'gza _ _ B-PER',\n",
              " ', _ _ O',\n",
              " 'country _ _ B-GRP',\n",
              " 'teasers _ _ I-GRP',\n",
              " 'and _ _ O',\n",
              " 'the _ _ B-PER',\n",
              " 'notorious _ _ I-PER',\n",
              " 'b.i.g. _ _ I-PER',\n",
              " '',\n",
              " '# id ff57d715-80cc-4fb2-9444-901a55ad5dc1\\tdomain=train',\n",
              " 'it _ _ O',\n",
              " 'is _ _ O',\n",
              " 'a _ _ O',\n",
              " 'series _ _ O',\n",
              " 'of _ _ O',\n",
              " 'badminton _ _ O',\n",
              " 'tournaments _ _ O',\n",
              " ', _ _ O',\n",
              " 'sanctioned _ _ O',\n",
              " 'by _ _ O',\n",
              " 'badminton _ _ B-GRP',\n",
              " 'world _ _ I-GRP',\n",
              " 'federation _ _ I-GRP']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/amzn/multiconer-baseline.git"
      ],
      "metadata": {
        "id": "LNmssnD0qwvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef41e54-5903-425e-b2a9-9e67fb8ed5f5"
      },
      "id": "LNmssnD0qwvY",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'multiconer-baseline'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 116 (delta 57), reused 66 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (116/116), 37.70 KiB | 7.54 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA READER**"
      ],
      "metadata": {
        "id": "SpqOxeeZtWUj"
      },
      "id": "SpqOxeeZtWUj"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/multiconer-baseline\")\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP26W2g2xc-L",
        "outputId": "c8d13eff-c0ab-417a-961d-7b8ab8007354"
      },
      "id": "YP26W2g2xc-L",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting absl-py==0.13.0\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 14.4 MB/s \n",
            "\u001b[?25hCollecting aiobotocore==1.3.0\n",
            "  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: alabaster==0.7.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.7.12)\n",
            "Collecting alembic==1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting allennlp==1.2.1\n",
            "  Downloading allennlp-1.2.1-py3-none-any.whl (504 kB)\n",
            "\u001b[K     |████████████████████████████████| 504 kB 39.2 MB/s \n",
            "\u001b[?25hCollecting argh==0.26.2\n",
            "  Downloading argh-0.26.2-py2.py3-none-any.whl (30 kB)\n",
            "Collecting async-generator==1.10\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting async-timeout==3.0.1\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting autovizwidget==0.19.0\n",
            "  Downloading autovizwidget-0.19.0.tar.gz (8.3 kB)\n",
            "Collecting backports.shutil-get-terminal-size==1.0.0\n",
            "  Downloading backports.shutil_get_terminal_size-1.0.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting bkcharts==0.2\n",
            "  Downloading bkcharts-0.2.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting blis==0.7.4\n",
            "  Downloading blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting brotlipy==0.7.0\n",
            "  Downloading brotlipy-0.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (929 kB)\n",
            "\u001b[K     |████████████████████████████████| 929 kB 60.4 MB/s \n",
            "\u001b[?25hCollecting bz2file==0.98\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Collecting catalogue==1.0.0\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 68.1 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer==2.0.3\n",
            "  Downloading charset_normalizer-2.0.3-py3-none-any.whl (35 kB)\n",
            "Collecting click==8.0.1\n",
            "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.5 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement clyent==1.2.2 (from versions: 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.3.2, 0.3.3, 0.3.4, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for clyent==1.2.2\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clyent==1.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khc8j5bzxnSf",
        "outputId": "fa02fe49-7b9b-463c-d508-a9bd58d0586b"
      },
      "id": "khc8j5bzxnSf",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clyent==1.2.0\n",
            "  Downloading clyent-1.2.0.tar.gz (20 kB)\n",
            "Building wheels for collected packages: clyent\n",
            "  Building wheel for clyent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clyent: filename=clyent-1.2.0-py3-none-any.whl size=9186 sha256=08945b0f006e4761a4df5b32a6873313c797ebd837061e90683c57378f61b87e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/f8/e7/bf537cfc0a25e2dab4c75f2ef63159e99ecb3c189fe88488b3\n",
            "Successfully built clyent\n",
            "Installing collected packages: clyent\n",
            "Successfully installed clyent-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/multiconer-baseline\")"
      ],
      "metadata": {
        "id": "x_3Dj-RE3mEU"
      },
      "id": "x_3Dj-RE3mEU",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wELzA3NM3s0k",
        "outputId": "1a3b7807-98ab-4683-a4cc-c7bfd0121e35"
      },
      "id": "wELzA3NM3s0k",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.10.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.9.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n",
            "Collecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.1+cu113)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.50.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.38.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.14.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning) (2.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=1bf1e68bbca2038704626dccec4d8a30be4205e6e2d6198917d03cf5518dd1be\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, lightning-utilities, pytorch-lightning\n",
            "Successfully installed fire-0.4.0 lightning-utilities-0.3.0 pytorch-lightning-1.8.1 torchmetrics-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1GWsH4Z31YR",
        "outputId": "adc48ed1-9489-49eb-e083-605ecb5f3bd3"
      },
      "id": "m1GWsH4Z31YR",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[K     |████████████████████████████████| 730 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.13.1+cu113)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 780 kB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.3)\n",
            "Collecting huggingface-hub>=0.0.16\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.0 MB/s \n",
            "\u001b[?25hCollecting lmdb>=1.2.1\n",
            "  Downloading lmdb-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting h5py>=3.6.0\n",
            "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 45.6 MB/s \n",
            "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.64.1)\n",
            "Collecting traitlets>5.1.1\n",
            "  Downloading traitlets-5.5.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 48.0 MB/s \n",
            "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
            "  Downloading spacy-3.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.6)\n",
            "Collecting base58>=2.1.1\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.19.6)\n",
            "Collecting fairscale==0.4.6\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 50.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.21.6)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.7)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.2)\n",
            "Collecting pytest>=6.2.5\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting filelock<3.8,>=3.3\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.5.0)\n",
            "Collecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.26.8-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting rich<13.0,>=12.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.8\n",
            "  Downloading botocore-1.29.8-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 53.8 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.8->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.56.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (2022.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.2-py3-none-any.whl (12 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (22.1.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.0.16->allennlp) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2022.9.24)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.6 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 64.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools, sacremoses\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=0b5a750fe0ebd6d57468dc74345291b7463dfe2f019cf972ac398e504e5c997d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=6217784f395b4c2f99b66cfa27b09302c46bf7e2f45feab7416c5019f70f7456\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp37-cp37m-linux_x86_64.whl size=3997190 sha256=0416e2c3c53421ee297e3af35241990d2c777ed24f8e3af4ae21dc2610590737\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/6b/48/a168ed5f8d01c50268605eff341c29126286763607bf707e3b\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0fdcd04a5d64f6f6ad27cb90dc9f0a3252866f39414e88066c1b9a4eb2f22e13\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=16de80d60ca819158df97c0139da9d71359df3ec684946367f0b6ebbb45b0257\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built fairscale termcolor jsonnet pathtools sacremoses\n",
            "Installing collected packages: urllib3, requests, jmespath, smmap, botocore, s3transfer, pydantic, gitdb, filelock, commonmark, tokenizers, thinc, shortuuid, setproctitle, sentry-sdk, rich, pluggy, pathtools, iniconfig, huggingface-hub, GitPython, exceptiongroup, docker-pycreds, boto3, wandb, transformers, traitlets, termcolor, tensorboardX, spacy, sentencepiece, sacremoses, pytest, lmdb, jsonnet, h5py, fairscale, cached-path, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.1.0\n",
            "    Uninstalling termcolor-2.1.0:\n",
            "      Successfully uninstalled termcolor-2.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 allennlp-2.10.1 base58-2.1.1 boto3-1.26.8 botocore-1.29.8 cached-path-1.1.6 commonmark-0.9.1 docker-pycreds-0.4.0 exceptiongroup-1.0.2 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.9 h5py-3.7.0 huggingface-hub-0.10.1 iniconfig-1.1.1 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.3.0 pathtools-0.1.2 pluggy-1.0.0 pydantic-1.8.2 pytest-7.2.0 requests-2.28.1 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.11.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.1 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 traitlets-5.5.0 transformers-4.20.1 urllib3-1.26.12 wandb-0.12.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install overrides"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxRQGtSl4d8w",
        "outputId": "fd709a67-62a2-4fe6-c9d3-53ecabae6653"
      },
      "id": "wxRQGtSl4d8w",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting overrides\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: overrides\n",
            "Successfully installed overrides-7.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***English Dataset***"
      ],
      "metadata": {
        "id": "K0mcIyPEibK0"
      },
      "id": "K0mcIyPEibK0"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m train_model --train /content/drive/MyDrive/NLP/EN-English/en_train.conll --dev /content/drive/MyDrive/NLP/EN-English/en_dev.conll --out_dir /content/drive/MyDrive/modelsmcn/english --model_name xlmr_ner --gpus 1 \\\n",
        "                                   --epochs 1 --encoder_model xlm-roberta-base --batch_size 256 --lr 0.0001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D9Mv1Orx1SX",
        "outputId": "b3691064-0b81-4b36-d4d0-9fde8e259722"
      },
      "id": "9D9Mv1Orx1SX",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:02:44 - INFO - reader - Reading file /content/drive/MyDrive/NLP/EN-English/en_train.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/EN-English/en_train.conll\n",
            "2022-11-14 15:03:54 - INFO - reader - Finished reading 15300 instances from file /content/drive/MyDrive/NLP/EN-English/en_train.conll\n",
            "INFO:root:Finished reading 15300 instances from file /content/drive/MyDrive/NLP/EN-English/en_train.conll\n",
            "2022-11-14 15:03:58 - INFO - reader - Reading file /content/drive/MyDrive/NLP/EN-English/en_dev.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/EN-English/en_dev.conll\n",
            "2022-11-14 15:04:01 - INFO - reader - Finished reading 800 instances from file /content/drive/MyDrive/NLP/EN-English/en_dev.conll\n",
            "INFO:root:Finished reading 800 instances from file /content/drive/MyDrive/NLP/EN-English/en_dev.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/drive/MyDrive/modelsmcn/english/xlmr_ner/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type                   | Params\n",
            "-------------------------------------------------------\n",
            "0 | encoder     | XLMRobertaModel        | 278 M \n",
            "1 | feedforward | Linear                 | 10.0 K\n",
            "2 | crf_layer   | ConditionalRandomField | 420   \n",
            "3 | dropout     | Dropout                | 0     \n",
            "-------------------------------------------------------\n",
            "278 M     Trainable params\n",
            "225       Non-trainable params\n",
            "278 M     Total params\n",
            "1,112.216 Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:03<00:00,  1.88s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   0% 0/64 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   2% 1/64 [00:03<03:21,  3.20s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/progress/base.py:250: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
            "  f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\n",
            "Epoch 0:  94% 60/64 [02:32<00:10,  2.54s/it, loss=16.90, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.00363, micro@R=0.00641, micro@F1=0.00464, MD@R=0.0204, MD@P=0.0116, MD@F1=0.0147, ALLTRUE=23552.0, ALLRECALLED=480.0, ALLPRED=41547.0]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 61/64 [02:34<00:07,  2.54s/it, loss=16.90, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.00363, micro@R=0.00641, micro@F1=0.00464, MD@R=0.0204, MD@P=0.0116, MD@F1=0.0147, ALLTRUE=23552.0, ALLRECALLED=480.0, ALLPRED=41547.0]\n",
            "Epoch 0:  97% 62/64 [02:36<00:05,  2.52s/it, loss=16.90, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.00363, micro@R=0.00641, micro@F1=0.00464, MD@R=0.0204, MD@P=0.0116, MD@F1=0.0147, ALLTRUE=23552.0, ALLRECALLED=480.0, ALLPRED=41547.0]\n",
            "Epoch 0:  98% 63/64 [02:37<00:02,  2.50s/it, loss=16.90, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.00363, micro@R=0.00641, micro@F1=0.00464, MD@R=0.0204, MD@P=0.0116, MD@F1=0.0147, ALLTRUE=23552.0, ALLRECALLED=480.0, ALLPRED=41547.0]\n",
            "Epoch 0: 100% 64/64 [02:37<00:00,  2.46s/it, loss=16.90, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.00363, micro@R=0.00641, micro@F1=0.00464, MD@R=0.0204, MD@P=0.0116, MD@F1=0.0147, ALLTRUE=23552.0, ALLRECALLED=480.0, ALLPRED=41547.0, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@PER=0.0348, val_R@PER=0.0519, val_F1@PER=0.0417, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@GRP=0.000, val_R@GRP=0.000, val_F1@GRP=0.000, val_P@CW=0.00157, val_R@CW=0.013, val_F1@CW=0.0028, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_micro@P=0.00816, val_micro@R=0.014, val_micro@F1=0.0103, val_MD@R=0.0387, val_MD@P=0.0226, val_MD@F1=0.0285, val_ALLTRUE=24782.0, val_ALLRECALLED=958.0, val_ALLPRED=42398.0, val_loss=20.70]\n",
            "                                                          \u001b[A/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Metric val_loss improved. New best score: 20.705\n",
            "Epoch 0: 100% 64/64 [02:37<00:00,  2.46s/it, loss=32.60, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@PER=0.0348, val_R@PER=0.0519, val_F1@PER=0.0417, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@GRP=0.000, val_R@GRP=0.000, val_F1@GRP=0.000, val_P@CW=0.00157, val_R@CW=0.013, val_F1@CW=0.0028, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_micro@P=0.00816, val_micro@R=0.014, val_micro@F1=0.0103, val_MD@R=0.0387, val_MD@P=0.0226, val_MD@F1=0.0285, val_ALLTRUE=24782.0, val_ALLRECALLED=958.0, val_ALLPRED=42398.0, val_loss=20.70]             tcmalloc: large alloc 1200185344 bytes == 0xda4de000 @  0x7f91f4063615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f5441 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 1733861376 bytes == 0x7f8b96a76000 @  0x7f91f40632a4 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f5441 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2167332864 bytes == 0x7f8b15788000 @  0x7f91f4063615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f54bc 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2709168128 bytes == 0xda4de000 @  0x7f91f4063615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f5441 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 3386466304 bytes == 0x7f8a4b9f2000 @  0x7f91f4063615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f54bc 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 4233084928 bytes == 0x7f894f4f6000 @  0x7f91f4063615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f91c842907f 0x7f91c83f9974 0x7f91a20faef5 0x7f91a20f5441 0x7f91a20fc549 0x7f91c83f9f3b 0x7f91c8083f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "Epoch 0: 100% 64/64 [03:08<00:00,  2.94s/it, loss=32.60, v_num=0, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@PER=0.0131, R@PER=0.0185, F1@PER=0.0154, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@GRP=0.000, R@GRP=0.000, F1@GRP=0.000, P@CW=0.00157, R@CW=0.0136, F1@CW=0.00281, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@PER=0.0348, val_R@PER=0.0519, val_F1@PER=0.0417, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@GRP=0.000, val_R@GRP=0.000, val_F1@GRP=0.000, val_P@CW=0.00157, val_R@CW=0.013, val_F1@CW=0.0028, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_micro@P=0.00816, val_micro@R=0.014, val_micro@F1=0.0103, val_MD@R=0.0387, val_MD@P=0.0226, val_MD@F1=0.0285, val_ALLTRUE=24782.0, val_ALLRECALLED=958.0, val_ALLPRED=42398.0, val_loss=20.70]\n",
            "2022-11-14 15:07:32 - INFO - utils - Stored model /content/drive/MyDrive/modelsmcn/english/xlmr_ner/lightning_logs/version_0/checkpoints//xlmr_ner_timestamp_1668438160.669594_final.ckpt.\n",
            "INFO:root:Stored model /content/drive/MyDrive/modelsmcn/english/xlmr_ner/lightning_logs/version_0/checkpoints//xlmr_ner_timestamp_1668438160.669594_final.ckpt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL_2Sh0Xy_J2",
        "outputId": "ca373d60-3140-4b67-9431-b4c98a4fa894"
      },
      "id": "sL_2Sh0Xy_J2",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/multiconer-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m evaluate --test /content/drive/MyDrive/NLP/EN-English/en_test.conll  --out_dir /content/ --gpus 1 --encoder_model xlm-roberta-base \\\n",
        "                                --model /content/xlmr_ner/lightning_logs/version_1/checkpoints/ --prefix xlmr_ner_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yzTNCIEx1VP",
        "outputId": "1e3ee2f3-9d0e-4259-b7ad-8718fce2afe2"
      },
      "id": "4yzTNCIEx1VP",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 14:20:31 - INFO - reader - Reading file /content/drive/MyDrive/NLP/EN-English/en_test.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/EN-English/en_test.conll\n",
            "2022-11-14 14:25:48 - INFO - reader - Finished reading 217818 instances from file /content/drive/MyDrive/NLP/EN-English/en_test.conll\n",
            "INFO:root:Finished reading 217818 instances from file /content/drive/MyDrive/NLP/EN-English/en_test.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing DataLoader 0:   0% 0/1702 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLTRUE', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLRECALLED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLPRED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1702/1702 [13:02<00:00,  2.18it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1702/1702 [13:02<00:00,  2.18it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLPRED         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        251751.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       ALLRECALLED       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        161320.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLTRUE         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        272901.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@CORP         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4167177379131317    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          F1@CW          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3060274124145508    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4666593670845032    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5635195374488831    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6645087003707886    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PROD         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3292565941810608    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@F1          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6149600148200989    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@P           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6407918930053711    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@R           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5911301374435425    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4592325687408447    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.35654985904693604   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5353884696960449    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5714186429977417    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.567250669002533    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4595752954483032    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.381407767534256    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2680458128452301    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.41356855630874634   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5558357834815979    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.802018940448761    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.25651758909225464   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    9.52439465281826     \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        micro@F1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4943734109401703    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@P         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5151399374008179    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@R         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4752162992954254    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "2022-11-14 14:38:58 - INFO - utils - Finished writing evaluation performance for /content//xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n",
            "INFO:root:Finished writing evaluation performance for /content//xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Hindi Dataset***"
      ],
      "metadata": {
        "id": "beo2JRlWiXsq"
      },
      "id": "beo2JRlWiXsq"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m train_model --train /content/drive/MyDrive/NLP/HI-Hindi/hi_train.conll --dev /content/drive/MyDrive/NLP/HI-Hindi/hi_dev.conll --out_dir /content/drive/MyDrive/modelsmcn/hindi --model_name xlmr_ner --gpus 1 \\\n",
        "                                   --epochs 1 --encoder_model xlm-roberta-base --batch_size 200 --lr 0.0001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPk0nWm_iXQr",
        "outputId": "2c5329e1-b116-4d79-e9e1-1ca477bbb879"
      },
      "id": "rPk0nWm_iXQr",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:08:26 - INFO - reader - Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_train.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_train.conll\n",
            "2022-11-14 15:09:23 - INFO - reader - Finished reading 15300 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_train.conll\n",
            "INFO:root:Finished reading 15300 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_train.conll\n",
            "2022-11-14 15:09:27 - INFO - reader - Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_dev.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_dev.conll\n",
            "2022-11-14 15:09:29 - INFO - reader - Finished reading 800 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_dev.conll\n",
            "INFO:root:Finished reading 800 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_dev.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/drive/MyDrive/modelsmcn/hindi/xlmr_ner/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type                   | Params\n",
            "-------------------------------------------------------\n",
            "0 | encoder     | XLMRobertaModel        | 278 M \n",
            "1 | feedforward | Linear                 | 10.0 K\n",
            "2 | crf_layer   | ConditionalRandomField | 420   \n",
            "3 | dropout     | Dropout                | 0     \n",
            "-------------------------------------------------------\n",
            "278 M     Trainable params\n",
            "225       Non-trainable params\n",
            "278 M     Total params\n",
            "1,112.216 Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:04<00:00,  2.08s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   0% 0/81 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   1% 1/81 [00:02<03:51,  2.89s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/progress/base.py:250: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
            "  f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\n",
            "Epoch 0:  95% 77/81 [02:47<00:08,  2.17s/it, loss=19.10, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.00904, micro@R=0.017, micro@F1=0.0118, MD@R=0.0433, MD@P=0.023, MD@F1=0.0301, ALLTRUE=15946.0, ALLRECALLED=690.0, ALLPRED=3e+4]     \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  96% 78/81 [02:49<00:06,  2.17s/it, loss=19.10, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.00904, micro@R=0.017, micro@F1=0.0118, MD@R=0.0433, MD@P=0.023, MD@F1=0.0301, ALLTRUE=15946.0, ALLRECALLED=690.0, ALLPRED=3e+4]\n",
            "Epoch 0:  98% 79/81 [02:50<00:04,  2.16s/it, loss=19.10, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.00904, micro@R=0.017, micro@F1=0.0118, MD@R=0.0433, MD@P=0.023, MD@F1=0.0301, ALLTRUE=15946.0, ALLRECALLED=690.0, ALLPRED=3e+4]\n",
            "Epoch 0:  99% 80/81 [02:51<00:02,  2.14s/it, loss=19.10, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.00904, micro@R=0.017, micro@F1=0.0118, MD@R=0.0433, MD@P=0.023, MD@F1=0.0301, ALLTRUE=15946.0, ALLRECALLED=690.0, ALLPRED=3e+4]\n",
            "Epoch 0: 100% 81/81 [02:52<00:00,  2.13s/it, loss=19.10, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.00904, micro@R=0.017, micro@F1=0.0118, MD@R=0.0433, MD@P=0.023, MD@F1=0.0301, ALLTRUE=15946.0, ALLRECALLED=690.0, ALLPRED=3e+4, val_P@GRP=0.170, val_R@GRP=0.0813, val_F1@GRP=0.110, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_P@PER=0.0397, val_R@PER=0.049, val_F1@PER=0.0438, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@CW=0.000566, val_R@CW=0.00579, val_F1@CW=0.00103, val_micro@P=0.0125, val_micro@R=0.0228, val_micro@F1=0.0162, val_MD@R=0.0581, val_MD@P=0.0319, val_MD@F1=0.0412, val_ALLTRUE=16773.0, val_ALLRECALLED=974.0, val_ALLPRED=30507.0, val_loss=17.30]\n",
            "                                                          \u001b[A/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Metric val_loss improved. New best score: 17.340\n",
            "Epoch 0: 100% 81/81 [02:52<00:00,  2.13s/it, loss=30.00, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@GRP=0.170, val_R@GRP=0.0813, val_F1@GRP=0.110, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_P@PER=0.0397, val_R@PER=0.049, val_F1@PER=0.0438, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@CW=0.000566, val_R@CW=0.00579, val_F1@CW=0.00103, val_micro@P=0.0125, val_micro@R=0.0228, val_micro@F1=0.0162, val_MD@R=0.0581, val_MD@P=0.0319, val_MD@F1=0.0412, val_ALLTRUE=16773.0, val_ALLRECALLED=974.0, val_ALLPRED=30507.0, val_loss=17.30]      tcmalloc: large alloc 1200185344 bytes == 0xda9d6000 @  0x7fe37ca7e615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab13441 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 1733861376 bytes == 0x7fdd4ea76000 @  0x7fe37ca7e2a4 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab13441 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2167332864 bytes == 0x7fdccd788000 @  0x7fe37ca7e615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab134bc 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2709168128 bytes == 0xda9d6000 @  0x7fe37ca7e615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab13441 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 3386466304 bytes == 0x7fdc039f2000 @  0x7fe37ca7e615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab134bc 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 4233084928 bytes == 0x7fdb074f6000 @  0x7fe37ca7e615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7fe350e4707f 0x7fe350e17974 0x7fe32ab18ef5 0x7fe32ab13441 0x7fe32ab1a549 0x7fe350e17f3b 0x7fe350aa1f61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "Epoch 0: 100% 81/81 [03:21<00:00,  2.49s/it, loss=30.00, v_num=0, P@GRP=0.148, R@GRP=0.0605, F1@GRP=0.086, P@PROD=0.000, R@PROD=0.000, F1@PROD=0.000, P@PER=0.0294, R@PER=0.0352, F1@PER=0.032, P@CORP=0.000, R@CORP=0.000, F1@CORP=0.000, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000566, R@CW=0.00608, F1@CW=0.00104, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@GRP=0.170, val_R@GRP=0.0813, val_F1@GRP=0.110, val_P@PROD=0.000, val_R@PROD=0.000, val_F1@PROD=0.000, val_P@PER=0.0397, val_R@PER=0.049, val_F1@PER=0.0438, val_P@CORP=0.000, val_R@CORP=0.000, val_F1@CORP=0.000, val_P@LOC=0.000, val_R@LOC=0.000, val_F1@LOC=0.000, val_P@CW=0.000566, val_R@CW=0.00579, val_F1@CW=0.00103, val_micro@P=0.0125, val_micro@R=0.0228, val_micro@F1=0.0162, val_MD@R=0.0581, val_MD@P=0.0319, val_MD@F1=0.0412, val_ALLTRUE=16773.0, val_ALLRECALLED=974.0, val_ALLPRED=30507.0, val_loss=17.30]\n",
            "2022-11-14 15:13:22 - INFO - utils - Stored model /content/drive/MyDrive/modelsmcn/hindi/xlmr_ner/lightning_logs/version_0/checkpoints//xlmr_ner_timestamp_1668438502.3457682_final.ckpt.\n",
            "INFO:root:Stored model /content/drive/MyDrive/modelsmcn/hindi/xlmr_ner/lightning_logs/version_0/checkpoints//xlmr_ner_timestamp_1668438502.3457682_final.ckpt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m evaluate --test /content/drive/MyDrive/NLP/HI-Hindi/hi_test.conll  --out_dir /content/drive/MyDrive/modelsmcn/hindi --gpus 1 --encoder_model xlm-roberta-base \\\n",
        "                                --model /content/xlmr_ner/lightning_logs/version_1/checkpoints/ --prefix xlmr_ner_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFccS9uLiOlm",
        "outputId": "17775f1d-2e3a-4928-9a8d-0dd1ada0551a"
      },
      "id": "wFccS9uLiOlm",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:23:06 - INFO - reader - Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_test.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/HI-Hindi/hi_test.conll\n",
            "2022-11-14 15:25:47 - INFO - reader - Finished reading 141565 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_test.conll\n",
            "INFO:root:Finished reading 141565 instances from file /content/drive/MyDrive/NLP/HI-Hindi/hi_test.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing DataLoader 0:   0% 0/1106 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLTRUE', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLRECALLED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLPRED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1106/1106 [08:21<00:00,  2.20it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1106/1106 [08:21<00:00,  2.20it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLPRED         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         99910.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       ALLRECALLED       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         36399.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLTRUE         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        144915.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@CORP         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12486514449119568   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          F1@CW          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06756935268640518   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12994550168514252   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1819932758808136    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2671046853065491    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PROD         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05385440215468407   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@F1          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2973470985889435    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@P           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3643178939819336    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@R           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.25117483735084534   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2554827630519867    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12036710232496262   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.29985302686691284   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3512876331806183    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18608511984348297   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24309788644313812   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08262331038713455   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04696753993630409   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.0829455628991127    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12280868738889694   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4730779826641083    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.030281376093626022   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   17.648226567245953    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        micro@F1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1731685847043991    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@P         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.21217095851898193   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@R         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14627885818481445   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "2022-11-14 15:34:36 - INFO - utils - Finished writing evaluation performance for /content/drive/MyDrive/modelsmcn/hindi/xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n",
            "INFO:root:Finished writing evaluation performance for /content/drive/MyDrive/modelsmcn/hindi/xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Bengali Dataset***"
      ],
      "metadata": {
        "id": "3hsCmSnmi5Z7"
      },
      "id": "3hsCmSnmi5Z7"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m train_model --train /content/drive/MyDrive/NLP/BN-Bangla/bn_train.conll --dev /content/drive/MyDrive/NLP/BN-Bangla/bn_dev.conll --out_dir /content/drive/MyDrive/modelsmcn/bengali --model_name xlmr_ner --gpus 1 \\\n",
        "                                   --epochs 1 --encoder_model xlm-roberta-base --batch_size 200 --lr 0.0001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m28JxY0i8hp",
        "outputId": "014d83a0-e0af-4a41-aa64-18f73a323db7"
      },
      "id": "9m28JxY0i8hp",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:16:17 - INFO - reader - Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_train.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_train.conll\n",
            "2022-11-14 15:17:10 - INFO - reader - Finished reading 15300 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_train.conll\n",
            "INFO:root:Finished reading 15300 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_train.conll\n",
            "2022-11-14 15:17:14 - INFO - reader - Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_dev.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_dev.conll\n",
            "2022-11-14 15:17:16 - INFO - reader - Finished reading 800 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_dev.conll\n",
            "INFO:root:Finished reading 800 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_dev.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type                   | Params\n",
            "-------------------------------------------------------\n",
            "0 | encoder     | XLMRobertaModel        | 278 M \n",
            "1 | feedforward | Linear                 | 10.0 K\n",
            "2 | crf_layer   | ConditionalRandomField | 420   \n",
            "3 | dropout     | Dropout                | 0     \n",
            "-------------------------------------------------------\n",
            "278 M     Trainable params\n",
            "225       Non-trainable params\n",
            "278 M     Total params\n",
            "1,112.216 Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:03<00:00,  1.67s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLTRUE', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLRECALLED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('val_ALLPRED', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   0% 0/81 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Epoch 0:   1% 1/81 [00:02<03:53,  2.92s/it]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/progress/base.py:250: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
            "  f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\n",
            "Epoch 0:  95% 77/81 [02:50<00:08,  2.21s/it, loss=21.80, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000261, micro@R=0.000196, micro@F1=0.000224, MD@R=0.00098, MD@P=0.0013, MD@F1=0.00112, ALLTRUE=15299.0, ALLRECALLED=15.00, ALLPRED=11513.0]   \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  96% 78/81 [02:52<00:06,  2.21s/it, loss=21.80, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000261, micro@R=0.000196, micro@F1=0.000224, MD@R=0.00098, MD@P=0.0013, MD@F1=0.00112, ALLTRUE=15299.0, ALLRECALLED=15.00, ALLPRED=11513.0]\n",
            "Epoch 0:  98% 79/81 [02:53<00:04,  2.20s/it, loss=21.80, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000261, micro@R=0.000196, micro@F1=0.000224, MD@R=0.00098, MD@P=0.0013, MD@F1=0.00112, ALLTRUE=15299.0, ALLRECALLED=15.00, ALLPRED=11513.0]\n",
            "Epoch 0:  99% 80/81 [02:54<00:02,  2.18s/it, loss=21.80, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000261, micro@R=0.000196, micro@F1=0.000224, MD@R=0.00098, MD@P=0.0013, MD@F1=0.00112, ALLTRUE=15299.0, ALLRECALLED=15.00, ALLPRED=11513.0]\n",
            "Epoch 0: 100% 81/81 [02:55<00:00,  2.17s/it, loss=21.80, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000261, micro@R=0.000196, micro@F1=0.000224, MD@R=0.00098, MD@P=0.0013, MD@F1=0.00112, ALLTRUE=15299.0, ALLRECALLED=15.00, ALLPRED=11513.0, val_P@PER=0.000, val_R@PER=0.000, val_F1@PER=0.000, val_P@CORP=0.0545, val_R@CORP=0.0132, val_F1@CORP=0.0213, val_P@LOC=0.00098, val_R@LOC=0.000816, val_F1@LOC=0.00089, val_P@CW=0.000, val_R@CW=0.000, val_F1@CW=0.000, val_P@GRP=0.00402, val_R@GRP=0.00951, val_F1@GRP=0.00566, val_P@PROD=0.000347, val_R@PROD=0.000297, val_F1@PROD=0.00032, val_micro@P=0.00511, val_micro@R=0.00391, val_micro@F1=0.00443, val_MD@R=0.0145, val_MD@P=0.0189, val_MD@F1=0.0164, val_ALLTRUE=16098.0, val_ALLRECALLED=233.0, val_ALLPRED=12325.0, val_loss=27.00]\n",
            "                                                          \u001b[A/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Metric val_loss improved. New best score: 27.045\n",
            "Epoch 0: 100% 81/81 [02:55<00:00,  2.17s/it, loss=36.20, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@PER=0.000, val_R@PER=0.000, val_F1@PER=0.000, val_P@CORP=0.0545, val_R@CORP=0.0132, val_F1@CORP=0.0213, val_P@LOC=0.00098, val_R@LOC=0.000816, val_F1@LOC=0.00089, val_P@CW=0.000, val_R@CW=0.000, val_F1@CW=0.000, val_P@GRP=0.00402, val_R@GRP=0.00951, val_F1@GRP=0.00566, val_P@PROD=0.000347, val_R@PROD=0.000297, val_F1@PROD=0.00032, val_micro@P=0.00511, val_micro@R=0.00391, val_micro@F1=0.00443, val_MD@R=0.0145, val_MD@P=0.0189, val_MD@F1=0.0164, val_ALLTRUE=16098.0, val_ALLRECALLED=233.0, val_ALLPRED=12325.0, val_loss=27.00]                  tcmalloc: large alloc 1200185344 bytes == 0xd910e000 @  0x7f2bef029615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc441 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 1733861376 bytes == 0x7f258aa76000 @  0x7f2bef0292a4 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc441 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2167332864 bytes == 0x7f2509788000 @  0x7f2bef029615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc4bc 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 2709168128 bytes == 0xd910e000 @  0x7f2bef029615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc441 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 3386466304 bytes == 0x7f243f9f2000 @  0x7f2bef029615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc4bc 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "tcmalloc: large alloc 4233084928 bytes == 0x7f23434f6000 @  0x7f2bef029615 0x58ead6 0x4f355e 0x58f8af 0x58fb26 0x7f2bc33f007f 0x7f2bc33c0974 0x7f2b9d0c1ef5 0x7f2b9d0bc441 0x7f2b9d0c3549 0x7f2bc33c0f3b 0x7f2bc304af61 0x58f6e4 0x590691 0x510946 0x5b575e 0x58ff2e 0x50c4fc 0x5b4ee6 0x58ff2e 0x510325 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e\n",
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "Epoch 0: 100% 81/81 [03:25<00:00,  2.54s/it, loss=36.20, v_num=1, P@PER=0.000, R@PER=0.000, F1@PER=0.000, P@CORP=0.0294, R@CORP=0.000385, F1@CORP=0.00076, P@LOC=0.000, R@LOC=0.000, F1@LOC=0.000, P@CW=0.000, R@CW=0.000, F1@CW=0.000, P@GRP=0.000172, R@GRP=0.000416, F1@GRP=0.000244, P@PROD=0.000347, R@PROD=0.000314, F1@PROD=0.00033, micro@P=0.000, micro@R=0.000, micro@F1=0.000, MD@R=0.000, MD@P=0.000, MD@F1=0.000, ALLTRUE=0.000, ALLRECALLED=0.000, ALLPRED=0.000, val_P@PER=0.000, val_R@PER=0.000, val_F1@PER=0.000, val_P@CORP=0.0545, val_R@CORP=0.0132, val_F1@CORP=0.0213, val_P@LOC=0.00098, val_R@LOC=0.000816, val_F1@LOC=0.00089, val_P@CW=0.000, val_R@CW=0.000, val_F1@CW=0.000, val_P@GRP=0.00402, val_R@GRP=0.00951, val_F1@GRP=0.00566, val_P@PROD=0.000347, val_R@PROD=0.000297, val_F1@PROD=0.00032, val_micro@P=0.00511, val_micro@R=0.00391, val_micro@F1=0.00443, val_MD@R=0.0145, val_MD@P=0.0189, val_MD@F1=0.0164, val_ALLTRUE=16098.0, val_ALLRECALLED=233.0, val_ALLPRED=12325.0, val_loss=27.00]\n",
            "2022-11-14 15:21:08 - INFO - utils - Stored model /content/drive/MyDrive/modelsmcn/bengali/xlmr_ner/lightning_logs/version_1/checkpoints//xlmr_ner_timestamp_1668438972.774039_final.ckpt.\n",
            "INFO:root:Stored model /content/drive/MyDrive/modelsmcn/bengali/xlmr_ner/lightning_logs/version_1/checkpoints//xlmr_ner_timestamp_1668438972.774039_final.ckpt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m evaluate --test /content/drive/MyDrive/NLP/BN-Bangla/bn_test.conll  --out_dir /content/drive/MyDrive/modelsmcn/bengali --gpus 1 --encoder_model xlm-roberta-base \\\n",
        "                                --model /content/xlmr_ner/lightning_logs/version_1/checkpoints/ --prefix xlmr_ner_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTRT5Orvi8lR",
        "outputId": "09ce3ad8-f8c5-4a7f-a0d6-8a551c8be34b"
      },
      "id": "hTRT5Orvi8lR",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:34:58 - INFO - reader - Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_test.conll\n",
            "INFO:root:Reading file /content/drive/MyDrive/NLP/BN-Bangla/bn_test.conll\n",
            "2022-11-14 15:37:11 - INFO - reader - Finished reading 133119 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_test.conll\n",
            "INFO:root:Finished reading 133119 instances from file /content/drive/MyDrive/NLP/BN-Bangla/bn_test.conll\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing DataLoader 0:   0% 0/1040 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLTRUE', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLRECALLED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('_tALLPRED', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1040/1040 [08:03<00:00,  2.15it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLTRUE', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLRECALLED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:235: UserWarning: You called `self.log('ALLPRED', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "Testing DataLoader 0: 100% 1040/1040 [08:03<00:00,  2.15it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLPRED         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         84113.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       ALLRECALLED       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         21752.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         ALLTRUE         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        135624.0         \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@CORP         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06774768978357315   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          F1@CW          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03208642452955246   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04069576784968376   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07769062370061874   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17110402882099152   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         F1@PROD         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.035282086580991745   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@F1          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19798213243484497   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@P           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.258604496717453    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          MD@R           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16038459539413452   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.22852233052253723   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11281178891658783   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20167286694049835   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1853596419095993    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          P@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11818703263998032   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         P@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15953756868839264   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@CORP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.0397687628865242    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@CW           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.018703008070588112   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@GRP          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02263127639889717   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@LOC          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04914436116814613   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          R@PER          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.30982479453086853   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         R@PROD          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.019834235310554504   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    26.19800581381871    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        micro@F1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10123010724782944   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@P         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1322268843650818    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         micro@R         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08200613409280777   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "2022-11-14 15:45:37 - INFO - utils - Finished writing evaluation performance for /content/drive/MyDrive/modelsmcn/bengali/xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n",
            "INFO:root:Finished writing evaluation performance for /content/drive/MyDrive/modelsmcn/bengali/xlmr_ner_results_base_xlmr_ner_timestamp_1668432077.6658995_final.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Multilingual Dataset(English+Hindi+Bengali)***"
      ],
      "metadata": {
        "id": "Aa08vZvvi8_v"
      },
      "id": "Aa08vZvvi8_v"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m train_model --train /content/drive/MyDrive/NLP/combined/com_train.conll --dev /content/drive/MyDrive/NLP/combined/com_dev.conll --out_dir /content/drive/MyDrive/modelsmcn/merged --model_name xlmr_ner --gpus 1 \\\n",
        "                                   --epochs 2 --encoder_model xlm-roberta-base --batch_size 256 --lr 0.0001"
      ],
      "metadata": {
        "id": "RxyDuap4jIpo"
      },
      "id": "RxyDuap4jIpo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m evaluate --test /content/drive/MyDrive/NLP/combined/com_test.conll  --out_dir /content/drive/MyDrive/modelsmcn/merged --gpus 1 --encoder_model xlm-roberta-base \\\n",
        "                                --model /content/xlmr_ner/lightning_logs/version_1/checkpoints/ --prefix xlmr_ner_results\n"
      ],
      "metadata": {
        "id": "zZ_hxOi2jPGT"
      },
      "id": "zZ_hxOi2jPGT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}